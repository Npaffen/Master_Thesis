@Article{Templ2011,
  author    = {Matthias Templ and Alexander Kowarik and Peter Filzmoser},
  journal   = {Computational Statistics {\&} Data Analysis},
  title     = {Iterative stepwise regression imputation using standard and robust methods},
  year      = {2011},
  month     = {oct},
  number    = {10},
  pages     = {2793--2806},
  volume    = {55},
  doi       = {10.1016/j.csda.2011.04.012},
  publisher = {Elsevier {BV}},
}

@Misc{Raghunathan01,
  author = {Trivellore E. Raghunathan and James M. Lepkowski and John Van Hoewyk and Peter Solenberger},
  title  = {A multivariate technique for multiply imputing missing values using a sequence of regression models. Survey Methodology 27},
  year   = {2001},
}

@Article{Karetnikov2019,
  author = {A.D. Karetnikov},
  title  = {Application of data-driven analytics on sport data from a professional bicycle racing team},
  year   = {2019},
  pages  = {1, 23, 24, 25, 47, 48, 51, 52, 53},
}

@InCollection{Kholkine2020,
  author    = {Leonid Kholkine and Tom De Schepper and Tim Verdonck and Steven Latr{\'{e}}},
  publisher = {Springer International Publishing},
  title     = {A Machine Learning Approach for Road Cycling Race Performance Prediction},
  year      = {2020},
  pages     = {103--112},
  doi       = {10.1007/978-3-030-64912-8_9},
}

@Article{Passfield2016,
  author    = {L. Passfield and JG. Hopker and S. Jobson and D. Friel and M. Zabala},
  title     = {Knowledge is power: Issues of measuring training and performance in cycling},
  year      = {2016},
  month     = {aug},
  number    = {14},
  pages     = {1426--1434},
  volume    = {35},
  doi       = {10.1080/02640414.2016.1215504},
  publisher = {Informa {UK} Limited},
}

@Article{Lemaitre2018,
  author  = {Lemaître, Guillaume and Lemaitre, Cedric},
  journal = {Science \& Cycling 2018 (Conference)},
  title   = {Estimate Power without Measuring it: a Machine Learning Application (Slides)},
  year    = {2018},
  doi     = {10.13140/RG.2.2.20151.57762},
}

@Article{Claudino2019a,
  author    = {Jo{\~{a}}o Gustavo Claudino and Daniel de Oliveira Capanema and Thiago Vieira de Souza and Julio Cerca Serr{\~{a}}o and Adriano C. Machado Pereira and George P. Nassis},
  title     = {Current Approaches to the Use of Artificial Intelligence for Injury Risk Assessment and Performance Prediction in Team Sports: a Systematic Review},
  year      = {2019},
  month     = {jul},
  number    = {1},
  volume    = {5},
  doi       = {10.1186/s40798-019-0202-3},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Tofallis2015,
  author    = {Chris Tofallis},
  title     = {A better measure of relative prediction accuracy for model selection and model estimation},
  year      = {2015},
  month     = {aug},
  number    = {8},
  pages     = {1352--1362},
  volume    = {66},
  doi       = {10.1057/jors.2014.103},
  publisher = {Informa {UK} Limited},
}

@Article{Strnad2015,
  author    = {Damjan Strnad and Andrej Nerat and {\v{S}}tefan Kohek},
  title     = {Neural network models for group behavior prediction: a case of soccer match attendance},
  year      = {2015},
  month     = {sep},
  number    = {2},
  pages     = {287--300},
  volume    = {28},
  doi       = {10.1007/s00521-015-2056-z},
  publisher = {Springer Science and Business Media {LLC}},
}

@Book{Ghatak2017,
  author    = {Ghatak, Abhijit},
  publisher = {Springer-Verlag GmbH},
  title     = {Machine Learning with R},
  year      = {2017},
  isbn      = {9789811068089},
  date      = {2017-11-23},
  ean       = {9789811068089},
  pagetotal = {210},
  url       = {https://www.ebook.de/de/product/33283026/abhijit_ghatak_machine_learning_with_r.html},
}

@InProceedings{Breiman1983,
  author = {Leo Breiman and Jerome H. Friedman and Richard A. Olshen and C. J. Stone},
  title  = {Classification and Regression Trees},
  year   = {1983},
}

@Misc{Therneau2019,
  author = {Terry M. Therneau and Elizabeth J. Atkinson},
  month  = apr,
  title  = {An Introduction to Recursive Partitioning Using the RPART Routines},
  year   = {2019},
}

@Article{Chen2016,
  author    = {Chen, Tianqi and Guestrin, Carlos},
  journal   = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  title     = {XGBoost},
  year      = {2016},
  month     = {Aug},
  doi       = {10.1145/2939672.2939785},
  publisher = {ACM},
  url       = {http://dx.doi.org/10.1145/2939672.2939785},
}

@Article{MicciBarreca2001,
  author     = {Micci-Barreca, Daniele},
  journal    = {SIGKDD Explor. Newsl.},
  title      = {A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems},
  year       = {2001},
  issn       = {1931-0145},
  month      = {jul},
  number     = {1},
  pages      = {27–32},
  volume     = {3},
  abstract   = {Categorical data fields characterized by a large number of distinct values represent a serious challenge for many classification and regression algorithms that require numerical inputs. On the other hand, these types of data fields are quite common in real-world data mining applications and often contain potentially relevant information that is difficult to represent for modeling purposes.This paper presents a simple preprocessing scheme for high-cardinality categorical data that allows this class of attributes to be used in predictive models such as neural networks, linear and logistic regression. The proposed method is based on a well-established statistical method (empirical Bayes) that is straightforward to implement as an in-database procedure. Furthermore, for categorical attributes with an inherent hierarchical structure, like ZIP codes, the preprocessing scheme can directly leverage the hierarchy by blending statistics at the various levels of aggregation.While the statistical methods discussed in this paper were first introduced in the mid 1950's, the use of these methods as a preprocessing step for complex models, like neural networks, has not been previously discussed in any literature.},
  address    = {New York, NY, USA},
  doi        = {10.1145/507533.507538},
  issue_date = {July 2001},
  keywords   = {neural networks, hierarchical attributes, categorical attributes, empirical bayes, predictive models},
  numpages   = {6},
  publisher  = {Association for Computing Machinery},
  url        = {https://doi.org/10.1145/507533.507538},
}

@Misc{Prokhorenkova2019,
  author        = {Liudmila Prokhorenkova and Gleb Gusev and Aleksandr Vorobev and Anna Veronika Dorogush and Andrey Gulin},
  title         = {CatBoost: unbiased boosting with categorical features},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1706.09516},
  primaryclass  = {cs.LG},
}

@Article{Borszcz2018,
  author    = {Fernando Borszcz and Artur Tramontin and Arthur Bossi and Lorival Carminatti and Vitor Costa},
  journal   = {International Journal of Sports Medicine},
  title     = {Functional Threshold Power in Cyclists: Validity of the Concept and Physiological Responses},
  year      = {2018},
  month     = {may},
  number    = {10},
  pages     = {737--742},
  volume    = {39},
  doi       = {10.1055/s-0044-101546},
  publisher = {Georg Thieme Verlag {KG}},
}

@Article{Vogt2006,
  author    = {Stefan Vogt and Lothar Heinrich and YORCK OLAF SCHUMACHER and ANDREAS BLUM and KAI ROECKER and HANS-HERMANN DICKHUTH and ANDREAS SCHMID},
  journal   = {Medicine {\&} Science in Sports {\&} Exercise},
  title     = {Power Output during Stage Racing in Professional Road Cycling},
  year      = {2006},
  month     = {jan},
  number    = {1},
  pages     = {147--151},
  volume    = {38},
  doi       = {10.1249/01.mss.0000183196.63081.6a},
  publisher = {Ovid Technologies (Wolters Kluwer Health)},
}

@Article{Breiman2001,
  author    = {Leo Breiman},
  journal   = {Machine Learning},
  title     = {Random Forests},
  year      = {2001},
  number    = {1},
  pages     = {5--32},
  volume    = {45},
  doi       = {10.1023/a:1010933404324},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Bergstra2012,
  author     = {Bergstra, James and Bengio, Yoshua},
  journal    = {J. Mach. Learn. Res.},
  title      = {Random Search for Hyper-Parameter Optimization},
  year       = {2012},
  issn       = {1532-4435},
  month      = {feb},
  number     = {null},
  pages      = {281–305},
  volume     = {13},
  abstract   = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
  issue_date = {3/1/2012},
  keywords   = {deep learning, global optimization, model selection, neural networks, response surface modeling},
  numpages   = {25},
  publisher  = {JMLR.org},
}

@InProceedings{pmlr-v133-turner21a,
  author    = {Turner, Ryan and Eriksson, David and McCourt, Michael and Kiili, Juha and Laaksonen, Eero and Xu, Zhen and Guyon, Isabelle},
  booktitle = {Proceedings of the NeurIPS 2020 Competition and Demonstration Track},
  title     = {Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning: Analysis of the Black-Box Optimization Challenge 2020},
  year      = {2021},
  editor    = {Escalante, Hugo Jair and Hofmann, Katja},
  month     = {06--12 Dec},
  pages     = {3--26},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {133},
  abstract  = {This paper presents the results and insights from the black-box optimization (BBO) challenge at NeurIPS2020 which ran from July–October, 2020. The challenge emphasized the importance of evaluating derivative-free optimizers for tuning the hyperparameters of machine learning models. This was the first black-box optimization challenge with a machine learning emphasis. It was based on tuning (validation set) performance of standard machine learning models on real datasets. This competition has widespread impact as black-box optimization (e.g., Bayesian optimization) is relevant for hyperparameter tuning in almost every machine learning project as well as many applications outside of machine learning. The final leaderboard was determined using the optimization performance on held-out (hidden) objective functions, where the optimizers ran without human intervention. Baselines were set using the default settings of several open source black-box optimization packages as well as random search.},
  pdf       = {http://proceedings.mlr.press/v133/turner21a/turner21a.pdf},
  url       = {https://proceedings.mlr.press/v133/turner21a.html},
}

@Article{Wilson2018,
  author      = {James T. Wilson and Frank Hutter and Marc Peter Deisenroth},
  title       = {Maximizing acquisition functions for Bayesian optimization},
  abstract    = {Bayesian optimization is a sample-efficient approach to global optimization that relies on theoretically motivated value heuristics (acquisition functions) to guide its search process. Fully maximizing acquisition functions produces the Bayes' decision rule, but this ideal is difficult to achieve since these functions are frequently non-trivial to optimize. This statement is especially true when evaluating queries in parallel, where acquisition functions are routinely non-convex, high-dimensional, and intractable. We first show that acquisition functions estimated via Monte Carlo integration are consistently amenable to gradient-based optimization. Subsequently, we identify a common family of acquisition functions, including EI and UCB, whose properties not only facilitate but justify use of greedy approaches for their maximization.},
  date        = {2018-05-25},
  eprint      = {1805.10196v2},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1805.10196v2:PDF},
  keywords    = {stat.ML, cs.LG},
}

@Article{Ranjan2011,
  author    = {Pritam Ranjan and Ronald Haynes and Richard Karsten},
  journal   = {Technometrics},
  title     = {A Computationally Stable Approach to Gaussian Process Interpolation of Deterministic Computer Simulation Data},
  year      = {2011},
  month     = {nov},
  number    = {4},
  pages     = {366--378},
  volume    = {53},
  doi       = {10.1198/tech.2011.09141},
  publisher = {Informa {UK} Limited},
}

@Article{Brochu2010,
  author      = {Eric Brochu and Vlad M. Cora and Nando de Freitas},
  title       = {A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning},
  abstract    = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
  date        = {2010-12-12},
  eprint      = {1012.2599v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1012.2599v1:PDF},
  keywords    = {cs.LG, G.1.6; G.3; I.2.6},
}

@Article{Snoek2012,
  author      = {Jasper Snoek and Hugo Larochelle and Ryan P. Adams},
  title       = {Practical Bayesian Optimization of Machine Learning Algorithms},
  abstract    = {Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
  date        = {2012-06-13},
  eprint      = {1206.2944v2},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1206.2944v2:PDF},
  keywords    = {stat.ML, cs.LG},
}

@Article{Shahhosseini2019,
  author      = {Mohsen Shahhosseini and Guiping Hu and Hieu Pham},
  title       = {Optimizing Ensemble Weights and Hyperparameters of Machine Learning Models for Regression Problems},
  abstract    = {Aggregating multiple learners through an ensemble of models aim to make better predictions by capturing the underlying distribution of the data more accurately. Different ensembling methods, such as bagging, boosting, and stacking/blending, have been studied and adopted extensively in research and practice. While bagging and boosting focus more on reducing variance and bias, respectively, stacking approaches target both by finding the optimal way to combine base learners. In stacking with the weighted average, ensembles are created from weighted averages of multiple base learners. It is known that tuning hyperparameters of each base learner inside the ensemble weight optimization process can produce better performing ensembles. To this end, an optimization-based nested algorithm that considers tuning hyperparameters as well as finding the optimal weights to combine ensembles (Generalized Weighted Ensemble with Internally Tuned Hyperparameters (GEM-ITH)) is designed. Besides, Bayesian search was used to speed-up the optimizing process, and a heuristic was implemented to generate diverse and well-performing base learners. The algorithm is shown to be generalizable to real data sets through analyses with ten publicly available data sets.},
  date        = {2019-08-14},
  eprint      = {1908.05287v6},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1908.05287v6:PDF},
  keywords    = {stat.ML, cs.LG, stat.ME},
}

@Article{Wright2016,
  author       = {Marvin N. Wright and Theresa Dankowski and Andreas Ziegler},
  title        = {Unbiased split variable selection for random survival forests using maximally selected rank statistics},
  abstract     = {The most popular approach for analyzing survival data is the Cox regression model. The Cox model may, however, be misspecified, and its proportionality assumption may not always be fulfilled. An alternative approach for survival prediction is random forests for survival outcomes. The standard split criterion for random survival forests is the log-rank test statistics, which favors splitting variables with many possible split points. Conditional inference forests avoid this split variable selection bias. However, linear rank statistics are utilized by default in conditional inference forests to select the optimal splitting variable, which cannot detect non-linear effects in the independent variables. An alternative is to use maximally selected rank statistics for the split point selection. As in conditional inference forests, splitting variables are compared on the p-value scale. However, instead of the conditional Monte-Carlo approach used in conditional inference forests, p-value approximations are employed. We describe several p-value approximations and the implementation of the proposed random forest approach. A simulation study demonstrates that unbiased split variable selection is possible. However, there is a trade-off between unbiased split variable selection and runtime. In benchmark studies of prediction performance on simulated and real datasets the new method performs better than random survival forests if informative dichotomous variables are combined with uninformative variables with more categories and better than conditional inference forests if non-linear covariate effects are included. In a runtime comparison the method proves to be computationally faster than both alternatives, if a simple p-value approximation is used.},
  date         = {2016-05-11},
  doi          = {10.1002/sim.7212},
  eprint       = {1605.03391v2},
  eprintclass  = {stat.ML},
  eprinttype   = {arXiv},
  file         = {:http\://arxiv.org/pdf/1605.03391v2:PDF},
  journaltitle = {Unbiased split variable selection for random survival forests using maximally selected rank statistics. Statistics in Medicine 36:1272-1284},
  keywords     = {stat.ML, cs.LG},
}

@InProceedings{Wang2020,
  author    = {Xiaogang Wang and Shichen Zhai and Jui-Le Chen},
  booktitle = {2020 8th International Conference on Orange Technology ({ICOT})},
  title     = {Research on House Price Forecast Based on Hyper Parameter Optimization Gradient Boosting Regression Model},
  year      = {2020},
  month     = {dec},
  publisher = {{IEEE}},
  doi       = {10.1109/icot51877.2020.9468726},
}

@InProceedings{Ke2017,
  author    = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  title     = {LightGBM: A Highly Efficient Gradient Boosting Decision Tree},
  year      = {2017},
  address   = {Red Hook, NY, USA},
  pages     = {3149–3157},
  publisher = {Curran Associates Inc.},
  series    = {NIPS'17},
  abstract  = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
  doi       = {10.5555/3294996.3295074},
  isbn      = {9781510860964},
  location  = {Long Beach, California, USA},
  numpages  = {9},
}

@Article{Lundberg2017,
  author      = {Scott Lundberg and Su-In Lee},
  title       = {A Unified Approach to Interpreting Model Predictions},
  abstract    = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  date        = {2017-05-22},
  eprint      = {1705.07874v2},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1705.07874v2:PDF},
  keywords    = {cs.AI, cs.LG, stat.ML},
}

@Article{Amoukou2021,
  author       = {Salim I. Amoukou and Nicolas J-B. Brunel and Tangi Salaün},
  title        = {Accurate Shapley Values for explaining tree-based models},
  abstract     = {Although Shapley Values (SV) are widely used in explainable AI, they can be poorly understood and estimated, implying that their analysis may lead to spurious inferences and explanations. As a starting point, we remind an invariance principle for SV and derive the correct approach for computing the SV of categorical variables that are particularly sensitive to the encoding used. In the case of tree-based models, we introduce two estimators of Shapley Values that exploit the tree structure efficiently and are more accurate than state-of-the-art methods. Simulations and comparisons are performed with state-of-the-art algorithms and show the practical gain of our approach. Finally, we discuss the ability of SV to provide reliable local explanations. We also provide a Python package that computes our estimators at https://github.com/salimamoukou/acv00.},
  date         = {2021-06-07},
  eprint       = {2106.03820v2},
  eprintclass  = {stat.ML},
  eprinttype   = {arXiv},
  file         = {:http\://arxiv.org/pdf/2106.03820v2:PDF},
  journaltitle = {AISTATS 2022},
  keywords     = {stat.ML, cs.LG},
}

@Article{Lundberg2018,
  author      = {Scott M. Lundberg and Gabriel G. Erion and Su-In Lee},
  title       = {Consistent Individualized Feature Attribution for Tree Ensembles},
  abstract    = {Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important, yet feature attribution for trees is often heuristic and not individualized for each prediction. Here we show that popular feature attribution methods are inconsistent, meaning they can lower a feature's assigned importance when the true impact of that feature actually increases. This is a fundamental problem that casts doubt on any comparison between features. To address it we turn to recent applications of game theory and develop fast exact tree solutions for SHAP (SHapley Additive exPlanation) values, which are the unique consistent and locally accurate attribution values. We then extend SHAP values to interaction effects and define SHAP interaction values. We propose a rich visualization of individualized feature attributions that improves over classic attribution summaries and partial dependence plots, and a unique "supervised" clustering (clustering based on feature attributions). We demonstrate better agreement with human intuition through a user study, exponential improvements in run time, improved clustering performance, and better identification of influential features. An implementation of our algorithm has also been merged into XGBoost and LightGBM, see http://github.com/slundberg/shap for details.},
  date        = {2018-02-12},
  eprint      = {1802.03888v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1802.03888v3:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Book{Brefeld2020,
  author    = {Brefeld ,Ulf and Davis, Jesse and Haaren, Jan Van and Zimmermann, Albrecht},
  publisher = {Springer International Publishing},
  title     = {Machine Learning and Data Mining for Sports Analytics},
  year      = {2020},
  date      = {2020-12-09},
  ean       = {9783030649128},
  pagetotal = {141},
  url       = {https://www.ebook.de/de/product/41245195/machine_learning_and_data_mining_for_sports_analytics.html},
}

@InProceedings{Hvarfner2022,
  author    = {Hvarfner, Carl and Stoll, Danny and Souza, Artur and Nardi, Luigi and Lindauer, Marius and Hutter, Frank},
  booktitle = {10th International Conference on Learning Representations, ICLR'22},
  title     = {piBO: Augmenting Acquisition Functions with User Beliefs for Bayesian Optimization},
  year      = {2022},
  month     = apr,
  pages     = {1-30},
  added-at  = {2022-02-22T10:57:58.000+0100},
  biburl    = {https://www.bibsonomy.org/bibtex/24da6eb29df4aa7572af576cb5e623be5/tntl3s},
  interhash = {7a49300f6471487c35ccad9f311c4282},
  intrahash = {4da6eb29df4aa7572af576cb5e623be5},
  keywords  = {Acquisition Augmenting Functions leibnizailab myown},
  timestamp = {2022-02-22T10:57:58.000+0100},
  url       = {https://openreview.net/pdf/1ce81b811a1cac6ed2405793a93e8512b1b50005.pdf},
}

@InCollection{Mignot2015,
  author    = {Jean-Fran{\c{c}}ois Mignot},
  booktitle = {The Economics of Professional Road Cycling},
  publisher = {Springer International Publishing},
  title     = {The History of Professional Road Cycling},
  year      = {2015},
  month     = {sep},
  pages     = {7--31},
  doi       = {10.1007/978-3-319-22312-4_2},
}

@InProceedings{Mason1999,
  author = {Mason, Llew and Baxter, Jonathan and Bartlett, Peter and Frean, Marcus},
  title  = {Boosting Algorithms as Gradient Descent.},
  year   = {1999},
  month  = {01},
  pages  = {512-518},
}

@Book{Shapley1952,
  author    = {Shapley, Lloyd S.},
  publisher = {RAND Corporation},
  title     = {A Value for N-Person Games},
  year      = {1952},
  address   = {Santa Monica, CA},
  doi       = {10.7249/P0295},
}

@Comment{jabref-meta: databaseType:bibtex;}
